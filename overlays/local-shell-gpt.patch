diff --git a/README.md b/README.md
index 25ec6d3..9825dad 100644
--- a/README.md
+++ b/README.md
@@ -9,7 +9,7 @@ pip install shell-gpt
 ```
 You'll need an OpenAI API key, you can generate one [here](https://beta.openai.com/account/api-keys).
 
-If the`$OPENAI_API_KEY` environment variable is set it will be used, otherwise, you will be prompted for your key which will then be stored in `~/.config/shell_gpt/.sgptrc`.
+If the`$LOCAL_AI_API_KEY` environment variable is set it will be used, otherwise, you will be prompted for your key which will then be stored in `~/.config/shell_gpt/.sgptrc`.
 
 ## Usage
 `sgpt` has a variety of use cases, including simple queries, shell queries, and code queries.
@@ -323,10 +323,10 @@ This is just some examples of what we can do using OpenAI GPT models, I'm sure y
 ### Runtime configuration file
 You can setup some parameters in runtime configuration file `~/.config/shell_gpt/.sgptrc`:
 ```text
-# API key, also it is possible to define OPENAI_API_KEY env.
-OPENAI_API_KEY=your_api_key
+# API key, also it is possible to define LOCAL_AI_API_KEY env.
+LOCAL_AI_API_KEY=your_api_key
 # OpenAI host, useful if you would like to use proxy.
-OPENAI_API_HOST=https://api.openai.com
+LOCAL_AI_API_HOST=https://api.openai.com
 # Max amount of cached message per chat session.
 CHAT_CACHE_LENGTH=100
 # Chat cache folder.
@@ -388,18 +388,16 @@ Switch `SYSTEM_ROLES` to force use [system roles](https://help.openai.com/en/art
 By default, ShellGPT leverages OpenAI's large language models. However, it also provides the flexibility to use locally hosted models, which can be a cost-effective alternative. To use local models, you will need to run your own API server. You can accomplish this by using [LocalAI](https://github.com/go-skynet/LocalAI), a self-hosted, OpenAI-compatible API. Setting up LocalAI allows you to run language models on your own hardware, potentially without the need for an internet connection, depending on your usage. To set up your LocalAI, please follow this comprehensive [guide](https://github.com/TheR1D/shell_gpt/wiki/LocalAI). Remember that the performance of your local models may depend on the specifications of your hardware and the specific language model you choose to deploy.
 
 ## Docker
-Run the container using the `OPENAI_API_KEY` environment variable, and a docker volume to store cache:
+Run the container using the `LOCAL_AI_API_KEY` environment variable, and a docker volume to store cache:
 ```shell
 docker run --rm \
-           --env OPENAI_API_KEY="your OPENAI API key" \
            --volume gpt-cache:/tmp/shell_gpt \
        ghcr.io/ther1d/shell_gpt --chat rainbow "what are the colors of a rainbow"
 ```
 
-Example of a conversation, using an alias and the `OPENAI_API_KEY` environment variable:
+Example of a conversation, using an alias and the `LOCAL_AI_API_KEY` environment variable:
 ```shell
-alias sgpt="docker run --rm --env OPENAI_API_KEY --volume gpt-cache:/tmp/shell_gpt ghcr.io/ther1d/shell_gpt"
-export OPENAI_API_KEY="your OPENAI API key"
+alias sgpt="docker run --rm --env LOCAL_AI_API_KEY --volume gpt-cache:/tmp/shell_gpt ghcr.io/ther1d/shell_gpt"
 sgpt --chat rainbow "what are the colors of a rainbow"
 sgpt --chat rainbow "inverse the list of your last answer"
 sgpt --chat rainbow "translate your last answer in french"
diff --git a/sgpt/app.py b/sgpt/app.py
index 0c97bda..28a474a 100644
--- a/sgpt/app.py
+++ b/sgpt/app.py
@@ -20,9 +20,9 @@ def main(
         show_default=False,
         help="The prompt to generate completions for.",
     ),
-    model: str = typer.Option(
+    model: str = typer.Argument(
         cfg.get("DEFAULT_MODEL"),
-        help="Large language model to use.",
+        help="GPT model to use.",
     ),
     temperature: float = typer.Option(
         0.1,
diff --git a/sgpt/config.py b/sgpt/config.py
index 45de530..de6698a 100644
--- a/sgpt/config.py
+++ b/sgpt/config.py
@@ -22,7 +22,8 @@ DEFAULT_CONFIG = {
     "CACHE_LENGTH": int(os.getenv("CHAT_CACHE_LENGTH", "100")),
     "REQUEST_TIMEOUT": int(os.getenv("REQUEST_TIMEOUT", "60")),
     "DEFAULT_MODEL": os.getenv("DEFAULT_MODEL", "gpt-3.5-turbo"),
-    "OPENAI_API_HOST": os.getenv("OPENAI_API_HOST", "https://api.openai.com"),
+    "LOCAL_AI_API_HOST": os.getenv("LOCAL_AI_API_HOST", "http://localhost:8080"),
+    "LOCAL_AI_API_KEY": os.getenv("LOCAL_AI_API_KEY", "na"),
     "DEFAULT_COLOR": os.getenv("DEFAULT_COLOR", "magenta"),
     "ROLE_STORAGE_PATH": os.getenv("ROLE_STORAGE_PATH", str(ROLE_STORAGE_PATH)),
     "SYSTEM_ROLES": os.getenv("SYSTEM_ROLES", "false"),
@@ -48,9 +49,9 @@ class Config(dict):  # type: ignore
         else:
             config_path.parent.mkdir(parents=True, exist_ok=True)
             # Don't write API key to config file if it is in the environment.
-            if not defaults.get("OPENAI_API_KEY") and not os.getenv("OPENAI_API_KEY"):
+            if not defaults.get("LOCAL_AI_API_KEY") and not os.getenv("LOCAL_AI_API_KEY"):
                 __api_key = getpass(prompt="Please enter your OpenAI API key: ")
-                defaults["OPENAI_API_KEY"] = __api_key
+                defaults["LOCAL_AI_API_KEY"] = __api_key
             super().__init__(**defaults)
             self._write()
 
diff --git a/sgpt/handlers/handler.py b/sgpt/handlers/handler.py
index f778e46..fb676e6 100644
--- a/sgpt/handlers/handler.py
+++ b/sgpt/handlers/handler.py
@@ -10,7 +10,7 @@ from ..role import SystemRole
 class Handler:
     def __init__(self, role: SystemRole) -> None:
         self.client = OpenAIClient(
-            cfg.get("OPENAI_API_HOST"), cfg.get("OPENAI_API_KEY")
+            cfg.get("LOCAL_AI_API_HOST"), cfg.get("LOCAL_AI_API_KEY")
         )
         self.role = role
         self.color = cfg.get("DEFAULT_COLOR")
diff --git a/tests/test_integration.py b/tests/test_integration.py
index 37b8fc2..06445d7 100644
--- a/tests/test_integration.py
+++ b/tests/test_integration.py
@@ -2,7 +2,7 @@
 This test module will execute real commands using shell.
 This means it will call sgpt.py with command line arguments.
 Make sure you have your API key in place ~/.cfg/shell_gpt/.sgptrc
-or ENV variable OPENAI_API_KEY.
+or ENV variable LOCAL_AI_API_KEY.
 It is useful for quick tests, saves a bit time.
 """
 
diff --git a/tests/test_unit.py b/tests/test_unit.py
index 888305c..6323962 100644
--- a/tests/test_unit.py
+++ b/tests/test_unit.py
@@ -8,12 +8,12 @@ from sgpt.client import OpenAIClient
 
 
 class TestMain(unittest.TestCase):
-    API_HOST = os.getenv("OPENAI_HOST", "https://api.openai.com")
+    API_HOST = os.getenv("LOCAL_AI_HOST", "https://api.openai.com")
     API_URL = f"{API_HOST}/v1/chat/completions"
     # TODO: Fix tests.
 
     def setUp(self):
-        self.api_key = os.environ["OPENAI_API_KEY"] = "test key"
+        self.api_key = os.environ["LOCAL_AI_API_KEY"] = "test key"
         self.prompt = "What is the capital of France?"
         self.shell = False
         self.execute = False
